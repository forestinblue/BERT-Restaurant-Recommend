# KeyBert
## keyword Extract using Korean KeyBERT

### 1. basic KeyBERT


```python
import numpy as np
import itertools
from konlpy.tag import Okt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer
```

I will use Korean documents related to food.

```python
doc = """[ ì‹ ì‚¬ ğŸ“ì¹´ë°”ë™ ]
-
ğŸ½íƒ€ë ˆì¹´ì¸ ë™ (11,000)
ğŸ½ê°€ë¼ì´ê·œë™ (11,500)
ğŸ½ì»µëˆ„ë“¤ (2,500)
ì‹ ì‚¬ì—­ ì‹ í¥ê°•ì ì¹´ë°”ë™... ê°€ë¡œìˆ˜ê¸¸ ê±´ë„ˆí¸ì— ìœ„ì¹˜í•´ ìˆì–´ ì¡°ìš©íˆ ì…ì†Œë¬¸ íƒ€ëŠ” ì¤‘ã…ã…
ë‚´ë¶€ëŠ” ë°”í…Œì´ë¸”ë¡œ ìë¦¬ê°€ ì¢€ ë„‰ë„‰í•œ í¸ì´ê³  íšŒì „ì´ ë¹¨ë¼ í˜„ì¬ëŠ” ì›¨ì´íŒ…ì´ ì—†ëŠ”ë“¯? 
ì‹œê·¸ë‹ˆì²˜ ë©”ë‰´ì¸ íƒ€ë ˆì¹´ì¸ ë™ì€ ëˆê¹ŒìŠ¤ì— ë‹¬ë‹¬í•œ íƒ€ë ˆì†ŒìŠ¤ ê·¸ë¦¬ê³  ê³„ë€ì´ë¶ˆì´ ë°¥ ìœ„ì— ìˆëŠ” êµ¬ì„±ìœ¼ë¡œ ëˆê¹ŒìŠ¤ëŸ¬ë²„ë¼ë©´ ì¢‹ì•„í•  ìˆ˜ ë°–ì— ã… 
ì—¬ê¸°ì„œ ê¿€íŒì€ ë¯¸ì†Œì¥êµ­ì„ ë‹›ì‹  ì»µëˆ„ë“¤ë¡œ ë³€ê²½í•˜ë©´ 2,500ì›ì— ê³ ê¸° ë“¤ì–´ê°„ ë¼ë©´ì„ ë¨¹ì„ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ!!\nê°€ë¼ì´ê·œë™ì€ ë§¤ì½¤í•œ ê·œë™ì¸ë° ë‚´ê°€ ë§ˆë¼ê¸°ë¦„ì„ ì¶”ê°€í•´ì„œ ê·¸ëŸ°ì§€ ìœ ë… ì œìœ¡ë³¶ìŒë§›ì´ ã…‹ã…‹ã…‹ã…‹ ë§›ìˆëŠ”ë° ë„˜ë‚˜ ì œìœ¡ìŠ¤ëŸ¬ì›Œì„œ ë‹¤ìŒì— ë°©ë¬¸í•˜ë©´ ê·¸ëƒ¥ íƒ€ë ˆì¹´ì¸ ë™ ì£¼ë¬¸í• ë“¯ã…ã…‹
ë‹¨ë¬´ì§€ë‘ ê°“ë¬´ì¹¨ ìœ ì ê³ ì¶”ì§€ê°€ í…Œì´ë¸”ë§ˆë‹¤ ìˆëŠ”ë° ê°“ë¬´ì¹¨ì€ ì§­ìª¼ë¦„í•œê²Œ ì€ê·¼ ë³„ë¯¸!! ìœ ì ê³ ì¶”ì§€ëŠ” ëˆê¹ŒìŠ¤ì— ì˜¬ë ¤ ë¨¹ìœ¼ë‹ˆ ì°°ë–¡ğŸ˜‹
ë²„í„° ì¶”ê°€í•´ì„œ ë°¥ì— ë¹„ë²¼ë¨¹ëŠ” ê²ƒë„ ì™•ì™• ë§›ìˆê³ ~
í•©ë¦¬ì ì¸ ê°€ê²©ìœ¼ë¡œ ì‹ ì‚¬ì—­ ê·¼ì²˜ì—ì„œ ë°¥ ë¨¹ê¸° ë„ˆë¬´ ì¢‹ì€ ê³³ì´ë¼ ë” í•«í•´ì§ˆ ê²ƒ ê°™ì€ ëŠë‚Œ!!
ëˆê¹ŒìŠ¤ ìì²´ëŠ” ì•„ì£¼ íŠ¹ë³„í•˜ì§€ ì•Šì§€ë§Œ ë‘íˆ¼í•œ ëˆê¹ŒìŠ¤ì™€ ê³¼í•˜ì§€ ì•Šê²Œ ë‹¬ì½¤ì§­ì§¤í•œ íƒ€ë ˆì†ŒìŠ¤ ê·¸ë¦¬ê³  ë¶€ë“œëŸ¬ìš´ ê³„ë€ê³¼ì˜ ì¡°í™”ê°€ ëƒ ëƒ ê¸‹ğŸ˜™

ğŸ‘‰íƒ€ë ˆì¹´ì¸ ë™ì€ ì¼ë°˜ê³¼ ìƒì´ ìˆëŠ”ë° ì–‘ì˜ ì°¨ì´ê°€ ì•„ë‹Œ ë“±ì‹¬ê³¼ ìƒë“±ì‹¬ì˜ ì°¨ì´. ê°€ê²©ì´ 4ì²œì›ì´ë‚˜ ì°¨ì´ê°€ ë‚˜ì§€ë§Œ í™•ì‹¤íˆ ìƒì´ ë” ì«„ê¹ƒí•˜ë©´ì„œ ë¶€ë“œëŸ¬ì›€
ğŸ‘‰ë°¥ ì¶”ê°€ëŠ” ë¬´ë£Œê¸° ë•Œë¬¸ì— ë²„í„°ë¥¼ 500ì›ì— ì¶”ê°€í•´ì„œ ìë¦¬ë§ˆë‹¤ ë¹„ì¹˜ëœ ê³„ë€ ê°„ì¥ì„ ë°¥ì— ë¿Œë ¤ ë¨¹ì!
"""
```

Create a document in which only nouns are extracted through a stemmer.
```python
okt = Okt()
tokenized_doc = okt.pos(doc)
tokenized_nouns = ' '.join([word[0] for word in tokenized_doc if word[1] == 'Noun'])
print('í’ˆì‚¬ íƒœê¹… 10ê°œë§Œ ì¶œë ¥ :',tokenized_doc[:10])
print('ëª…ì‚¬ ì¶”ì¶œ :',tokenized_nouns)
```

```python
í’ˆì‚¬ íƒœê¹… 10ê°œë§Œ ì¶œë ¥ : [('[', 'Punctuation'), ('ì‹ ì‚¬', 'Noun'), ('ğŸ“', 'Foreign'), ('ì¹´ë°”', 'Noun'), ('ë™', 'Modifier'),
(']', 'Punctuation'),('\n', 'Foreign'), ('-', 'Punctuation'), ('\n', 'Foreign'), ('ğŸ½', 'Foreign')]
ëª…ì‚¬ ì¶”ì¶œ : ì‹ ì‚¬ ì¹´ë°” ë ˆ ì¹´ì¸ ë™ ê°€ë¼ì´ ê·œë™ ì»µëˆ„ ì‹ ì‚¬ì—­ ì‹ í¥ ê°• ì¹´ë°” ê°€ë¡œìˆ˜ê¸¸ ê±´ë„ˆí¸ ìœ„ì¹˜ ì… ì†Œë¬¸ ì¤‘ ë‚´ë¶€ ë°” í…Œì´ë¸” ìë¦¬ ì¢€ í¸ì´ íšŒì „ í˜„ì¬ ì›¨ì´íŒ… ì²˜ ë©”ë‰´ì¸ ë ˆ ì¹´ì¸ ë™ 
ëˆê¹ŒìŠ¤ ë‹¬ë‹¬ ë ˆì†ŒìŠ¤ ê³„ë€ ì´ë¶ˆ ë°¥ ìœ„ êµ¬ì„± ëˆê¹ŒìŠ¤ ëŸ¬ë²„ ë¼ë©´ ìˆ˜ ì—¬ê¸° ê¿€íŒ ë¯¸ì†Œ ì¥êµ­ ë‹›ì‹  ì»µëˆ„ ë³€ê²½ ê³ ê¸° ë¼ë©´ ìˆ˜ ê²ƒ ê°€ë¼ì´ ê·œë™ ë§¤ì½¤ ê·œë™ ë‚´ ê¸°ë¦„ ì¶”ê°€ ìœ ë… ë³¶ìŒ ë§› ì œìœ¡ ë‹¤ìŒ 
ë°©ë¬¸ ê·¸ëƒ¥ ë ˆ ì¹´ì¸ ë™ ì£¼ë¬¸ ë‹¨ë¬´ì§€ ê°“ ìœ ì ê³ ì¶” í…Œì´ë¸” ê°“ ì§­ìª¼ë¦„í•œ ì€ê·¼ ë³„ë¯¸ ìœ ì ê³ ì¶” ëˆê¹ŒìŠ¤ ì°°ë–¡ ë²„í„° ì¶”ê°€ ë°¥ ê²ƒ ì™•ì™• í•©ë¦¬ ê°€ê²© ì‹ ì‚¬ì—­ ê·¼ì²˜ ë°¥ ë¨¹ê¸° ê³³ ë” í•« ê²ƒ ëŠë‚Œ ëˆê¹ŒìŠ¤ ìì²´ 
ì•„ì£¼ ë‘íˆ¼ ëˆê¹ŒìŠ¤ ê³¼ ë‹¬ì½¤ ë ˆì†ŒìŠ¤ ê³„ë€ ì¡°í™” ë ˆ ì¹´ì¸ ë™ ì¼ë°˜ ìƒì´ ì–‘ ì°¨ì´ ë“±ì‹¬ ìƒë“± ì‹¬ì˜ ì°¨ì´ ê°€ê²© ì°¨ì´ ë‚˜ ìƒì´ ë” ì«„ê¹ƒ ë¶€ë“œëŸ¬ì›€ ë°¥ ì¶”ê°€ ë¬´ë£Œ ê¸° ë•Œë¬¸ ë²„í„° ì¶”ê°€ ìë¦¬ ë¹„ì¹˜ ê³„ë€ ê°„ì¥ë°¥
```





Use sklearn CountVectorizer to extract words.
The reason for using CountVectorizer is, can easily extract n-grams by using the argument of n_gram_range. 
```python
n_gram_range = (2, 3)

count = CountVectorizer(ngram_range=n_gram_range).fit([tokenized_nouns])
candidates = count.get_feature_names()

print('trigram ê°œìˆ˜ :',len(candidates))
print('trigram ë‹¤ì„¯ê°œë§Œ ì¶œë ¥ :',candidates[:5])
```
```python
trigram ê°œìˆ˜ : 203
trigram ë‹¤ì„¯ê°œë§Œ ì¶œë ¥ : ['ê°€ê²© ì‹ ì‚¬ì—­', 'ê°€ê²© ì‹ ì‚¬ì—­ ê·¼ì²˜', 'ê°€ê²© ì°¨ì´', 'ê°€ê²© ì°¨ì´ ìƒì´', 'ê°€ë¼ì´ ê·œë™']
```


Numericalize keywords through SBERT
```python
model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')
doc_embedding = model.encode([doc])
candidate_embeddings = model.encode(candidates)

top_n = 5
distances = cosine_similarity(doc_embedding, candidate_embeddings)
keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]
print(keywords)
```

### 2. Max Sum Similarity
> [information about Max Sum Similarity](https://aclanthology.org/P08-1007.pdf)

```python
def max_sum_sim(doc_embedding, candidate_embeddings, words, top_n, nr_candidates):
    # ë¬¸ì„œì™€ ê° í‚¤ì›Œë“œë“¤ ê°„ì˜ ìœ ì‚¬ë„
    distances = cosine_similarity(doc_embedding, candidate_embeddings)

    # ê° í‚¤ì›Œë“œë“¤ ê°„ì˜ ìœ ì‚¬ë„
    distances_candidates = cosine_similarity(candidate_embeddings, 
                                            candidate_embeddings)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ì— ê¸°ë°˜í•˜ì—¬ í‚¤ì›Œë“œë“¤ ì¤‘ ìƒìœ„ top_nê°œì˜ ë‹¨ì–´ë¥¼ pick.
    words_idx = list(distances.argsort()[0][-nr_candidates:])
    words_vals = [candidates[index] for index in words_idx]
    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]

    # ê° í‚¤ì›Œë“œë“¤ ì¤‘ì—ì„œ ê°€ì¥ ëœ ìœ ì‚¬í•œ í‚¤ì›Œë“œë“¤ê°„ì˜ ì¡°í•©ì„ ê³„ì‚°
    min_sim = np.inf
    candidate = None
    for combination in itertools.combinations(range(len(words_idx)), top_n):
        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])
        if sim < min_sim:
            candidate = combination
            min_sim = sim

    return [words_vals[idx] for idx in candidate]
```

A relatively high number of nr_candidates creates a variety of keywords.
```python
top_n = 5
distances = cosine_similarity(doc_embedding, candidate_embeddings)
keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]
print(keywords)
```
```python
['ìœ ë… ë³¶ìŒ ì œìœ¡', 'ë ˆì†ŒìŠ¤ ê³„ë€ ì´ë¶ˆ', 'í…Œì´ë¸” ì§­ìª¼ë¦„í•œ ì€ê·¼', 'ë‹¬ë‹¬ ë ˆì†ŒìŠ¤ ê³„ë€', 'ë‹¬ì½¤ ë ˆì†ŒìŠ¤ ê³„ë€']
```
